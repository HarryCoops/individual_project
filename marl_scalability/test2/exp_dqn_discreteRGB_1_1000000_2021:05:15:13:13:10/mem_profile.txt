Filename: marl_scalability/train.py

Line #    Mem usage    Increment  Occurences   Line Contents
============================================================
    62    456.5 MiB    456.5 MiB           1       @profile(stream=f)
    63                                             def train(
    64                                                 scenario,
    65                                                 n_agents,
    66                                                 num_episodes,
    67                                                 max_episode_steps,
    68                                                 eval_info,
    69                                                 timestep_sec,
    70                                                 headless,
    71                                                 policy_class,
    72                                                 seed,
    73                                                 log_dir,
    74                                                 experiment_name,
    75                                                 record_vehicle_lifespan,
    76                                                 record_mem_usage,
    77                                                 max_steps
    78                                             ):
    79    456.5 MiB      0.0 MiB           1           torch.set_num_threads(1)
    80    456.5 MiB      0.0 MiB           1           total_step = 0
    81    456.5 MiB      0.0 MiB           1           finished = False
    82                                                 # Make agent_ids in the form of 000, 001, ..., 010, 011, ..., 999, 1000, ...;
    83    456.5 MiB      0.0 MiB           4           agent_ids = ["0" * max(0, 3 - len(str(i))) + str(i) for i in range(n_agents)]
    84                                         
    85                                                 # Assign the agent classes
    86    456.5 MiB      0.0 MiB           4           agent_classes = {
    87                                                     agent_id: policy_class
    88    456.5 MiB      0.0 MiB           2               for agent_id in agent_ids
    89                                                 }
    90                                                 # Create the agent specifications matched with their associated ID.
    91    534.8 MiB      0.0 MiB           4           agent_specs = {
    92                                                     agent_id: make(locator=policy_class, max_episode_steps=max_episode_steps)
    93    534.8 MiB     78.3 MiB           2               for agent_id, policy_class in agent_classes.items()
    94                                                 }
    95                                                 # Create the agents matched with their associated ID.
    96    594.2 MiB      0.0 MiB           4           agents = {
    97                                                     agent_id: agent_spec.build_agent()
    98    594.2 MiB     59.4 MiB           2               for agent_id, agent_spec in agent_specs.items()
    99                                                 }
   100    594.2 MiB      0.0 MiB           1           print(list(agents.values())[0])
   101                                                 # Create the environment.
   102    594.2 MiB      0.0 MiB           1           env = gym.make(
   103    594.2 MiB      0.0 MiB           1               "marl_scalability.env:scalability-v0",
   104    594.2 MiB      0.0 MiB           1               agent_specs=agent_specs,
   105    594.2 MiB      0.0 MiB           1               scenarios=[scenario,],
   106    594.2 MiB      0.0 MiB           1               headless=headless,
   107    594.2 MiB      0.0 MiB           1               timestep_sec=0.1,
   108    673.5 MiB     79.3 MiB           1               seed=seed,
   109                                                 )
   110                                         
   111                                                 # Define an 'etag' for this experiment's data directory based off policy_classes.
   112                                                 # E.g. From a ["marl_scalability.baselines.dqn:dqn-v0", "marl_scalability.baselines.ppo:ppo-v0"]
   113                                                 # policy_classes list, transform it to an etag of "dqn-v0:ppo-v0".
   114                                                 #etag = ":".join([policy_class.split(":")[-1] for policy_class in policy_classes])
   115    673.5 MiB      0.0 MiB           1           surviving_vehicles_total = []
   116    673.5 MiB      0.0 MiB           1           mem_usage = []
   117    673.5 MiB      0.0 MiB           1           mem_usage_interval = 10
   118   1579.1 MiB      0.0 MiB           2           for episode in episodes(num_episodes, experiment_name=experiment_name, log_dir=log_dir, write_table=True):
   119                                                     # Reset the environment and retrieve the initial observations.
   120   1579.1 MiB      0.0 MiB           2               surviving_vehicles = []
   121   1588.7 MiB     46.3 MiB           2               observations = env.reset()
   122   1588.7 MiB      0.0 MiB           2               dones = {"__all__": False}
   123   1588.7 MiB      0.0 MiB           2               infos = None
   124   1588.7 MiB      0.0 MiB           2               episode.reset()
   125   1588.7 MiB      0.0 MiB           2               experiment_dir = episode.experiment_dir
   126                                                     # Save relevant agent metadata.
   127   1588.7 MiB      0.0 MiB           2               if not os.path.exists(f"{experiment_dir}/agent_metadata.pkl"):
   128    710.3 MiB      0.0 MiB           1                   if not os.path.exists(experiment_dir):
   129                                                             os.makedirs(experiment_dir)
   130    710.3 MiB      0.0 MiB           1                   with open(f"{experiment_dir}/agent_metadata.pkl", "wb") as metadata_file:
   131    710.3 MiB      0.0 MiB           1                       dill.dump(
   132                                                                 {
   133    710.3 MiB      0.0 MiB           1                               "agent_ids": agent_ids,
   134    710.3 MiB      0.0 MiB           1                               "agent_classes": agent_classes,
   135    710.3 MiB      0.0 MiB           1                               "agent_specs": agent_specs,
   136                                                                 },
   137    710.3 MiB      0.0 MiB           1                           metadata_file,
   138    710.3 MiB      0.0 MiB           1                           pickle.HIGHEST_PROTOCOL,
   139                                                             )
   140                                         
   141   1694.0 MiB -43296.0 MiB        1001               while not dones["__all__"]:
   142                                                         # Break if any of the agent's step counts is 1000000 or greater.
   143   1694.0 MiB -173145.8 MiB        4000                   if any([episode.get_itr(agent_id) >= 1000000 for agent_id in agents]):
   144                                                             finished = True
   145                                                             break
   146                                         
   147                                                         # Perform the evaluation check.
   148                                                         '''
   149                                                         evaluation_check(
   150                                                             agents=agents,
   151                                                             agent_ids=agent_ids,
   152                                                             policy_classes=agent_classes,
   153                                                             episode=episode,
   154                                                             log_dir=log_dir,
   155                                                             max_episode_steps=max_episode_steps,
   156                                                             **eval_info,
   157                                                             **env.info,
   158                                                         )
   159                                                         '''
   160                                         
   161                                                         # Request and perform actions on each agent that received an observation.
   162   1694.0 MiB -173145.8 MiB        4000                   actions = {
   163                                                             agent_id: agents[agent_id].act(observation, explore=True)
   164   1694.0 MiB -86572.5 MiB        2000                       for agent_id, observation in observations.items()
   165                                                         }
   166   1694.0 MiB -43282.5 MiB        1000                   next_observations, rewards, dones, infos = env.step(actions)
   167                                                         # Active agents are those that receive observations in this step and the next
   168                                                         # step. Step each active agent (obtaining their network loss if applicable).
   169   1694.0 MiB -43285.9 MiB        1000                   active_agent_ids = observations.keys() & next_observations.keys()
   170   1694.0 MiB -43286.0 MiB        1000                   surviving_vehicles.append(len(active_agent_ids))
   171   1694.0 MiB -172932.0 MiB        4000                   loss_outputs = {
   172                                                             agent_id: agents[agent_id].step(
   173                                                                 state=observations[agent_id],
   174                                                                 action=actions[agent_id],
   175                                                                 reward=rewards[agent_id],
   176                                                                 next_state=next_observations[agent_id],
   177                                                                 done=dones[agent_id],
   178                                                                 info=infos[agent_id],
   179                                                             )
   180   1694.0 MiB -85168.4 MiB        2000                       for agent_id in active_agent_ids
   181                                                         }
   182                                         
   183                                                         # Record the data from this episode.
   184   1694.0 MiB -43074.0 MiB        1000                   episode.record_step(
   185   1694.0 MiB -43074.0 MiB        1000                       agent_ids_to_record=active_agent_ids,
   186   1694.0 MiB -43074.0 MiB        1000                       infos=infos,
   187   1694.0 MiB -43074.0 MiB        1000                       rewards=rewards,
   188   1694.0 MiB -43074.0 MiB        1000                       total_step=total_step,
   189   1694.0 MiB -43073.9 MiB        1000                       loss_outputs=loss_outputs,
   190                                                         )
   191                                                         # Update variables for the next step.
   192   1694.0 MiB -43074.0 MiB        1000                   total_step += 1
   193   1694.0 MiB -43401.4 MiB        1000                   observations = next_observations
   194                                                         
   195   1694.0 MiB -43401.4 MiB        1000                   if total_step % mem_usage_interval == 0:
   196   1632.5 MiB  -4382.6 MiB         100                       process = psutil.Process(os.getpid())
   197   1632.5 MiB  -5072.0 MiB         400                       replay_buffer_mem_usage = sum(sys.getsizeof(p.replay)
   198   1632.5 MiB  -3803.9 MiB         300                           for p in agents.values()
   199                                                             )
   200   1632.5 MiB  -1268.0 MiB         100                       mem_usage.append(
   201   1632.5 MiB  -1268.0 MiB         100                           (total_step, process.memory_info().rss, replay_buffer_mem_usage)
   202                                                             )
   203   1694.0 MiB -40286.8 MiB        1000                   if max_steps and total_step >= max_steps:
   204   1588.7 MiB   -105.3 MiB           1                       finished = True
   205   1588.7 MiB      0.0 MiB           1                       break
   206                                         
   207                                                     # Normalize the data and record this episode on tensorboard.
   208   1588.7 MiB   -114.9 MiB           2               episode.record_episode()
   209   1588.7 MiB      0.0 MiB           2               episode.record_tensorboard()
   210   1588.7 MiB      0.0 MiB           2               surviving_vehicles += [0,] * (max_episode_steps - len(surviving_vehicles))
   211   1588.7 MiB      0.0 MiB           2               surviving_vehicles_total.append(surviving_vehicles)
   212   1588.7 MiB      0.0 MiB           2               if finished:
   213   1588.7 MiB      0.0 MiB           1                   break
   214   1588.7 MiB      0.0 MiB           1           if record_vehicle_lifespan:
   215                                                     with open(Path(log_dir) / experiment_name / "surviving_vehicle_data.csv", "w") as f:
   216                                                         writer = csv.writer(f)
   217                                                         writer.writerows(surviving_vehicles_total)
   218   1588.7 MiB      0.0 MiB           1           if record_mem_usage:
   219   1588.7 MiB      0.0 MiB           1               mem_usage, steps, replay_usage = zip(*mem_usage)
   220   1588.7 MiB      0.0 MiB           1               mem_usage = pd.DataFrame(
   221                                                         {
   222   1589.2 MiB      0.5 MiB           1                       "mem_usage": pd.Series(mem_usage), 
   223   1589.2 MiB      0.0 MiB           1                       "step": pd.Series(steps),
   224   1589.4 MiB      0.2 MiB           1                       "replay_usage": pd.Series(replay_usage)
   225                                                         }
   226                                                     )
   227   1589.8 MiB      0.4 MiB           1               mem_usage.to_csv(Path(log_dir) / experiment_name / "mem_usage.csv")
   228   1573.5 MiB    -16.3 MiB           1           env.close()


