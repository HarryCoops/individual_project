Timer unit: 1e-06 s

Total time: 3.80199 s
File: /marl/marl_scalability/baselines/common/multi_agent_image_replay_buffer.py
Function: generate_samples at line 214

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   214                                               @profile 
   215                                               def generate_samples(self):
   216         5          8.0      1.6      0.0          n_agents = len(self.needs_sample)
   217         5         11.0      2.2      0.0          image_data = torch.empty(
   218         5     342119.0  68423.8      9.0              (self.batch_size * n_agents, *self.dimensions), pin_memory=True
   219                                                   )
   220         5         11.0      2.2      0.0          low_dim_data = torch.empty(
   221         5       1408.0    281.6      0.0              (self.batch_size * n_agents, 12), pin_memory=True
   222                                                   )
   223         5          5.0      1.0      0.0          all_actions = torch.empty(
   224         5         43.0      8.6      0.0              (self.batch_size * n_agents, 2), pin_memory=True
   225                                                   )
   226         5          6.0      1.2      0.0          all_rewards = torch.empty(
   227         5         40.0      8.0      0.0              (self.batch_size * n_agents, 1), pin_memory=True
   228                                                   )
   229         5          4.0      0.8      0.0          next_image_data = torch.empty(
   230         5     344289.0  68857.8      9.1              (self.batch_size * n_agents, *self.dimensions), pin_memory=True
   231                                                   )
   232         5         10.0      2.0      0.0          next_low_dim_data = torch.empty(
   233         5        233.0     46.6      0.0              (self.batch_size * n_agents, 11), pin_memory=True
   234                                                   )
   235         5          6.0      1.2      0.0          all_dones = torch.empty(
   236         5         49.0      9.8      0.0              (self.batch_size * n_agents, 1), pin_memory=True
   237                                                   )
   238       105        259.0      2.5      0.0          for i, agent_id in enumerate(self.needs_sample):
   239       100    2143257.0  21432.6     56.4              agent_batch = list(iter(self.agent_dataloaders[agent_id]))
   240       100       3708.0     37.1      0.1              states, actions, rewards, next_states, dones, others = zip(*agent_batch)
   241      3300       3864.0      1.2      0.1              for j in range(self.batch_size):
   242      3200     431836.0    134.9     11.4                  image_data[i*self.batch_size+j] = states[j]["top_down_rgb"]
   243      3200      31005.0      9.7      0.8                  low_dim_data[i*self.batch_size+j] = states[j]["low_dim_states"]
   244      3200     359333.0    112.3      9.5                  next_image_data[i*self.batch_size+j] = next_states[j]["top_down_rgb"]
   245      3200      29270.0      9.1      0.8                  next_low_dim_data[i*self.batch_size+j] = next_states[j]["low_dim_states"]
   246      3200      21825.0      6.8      0.6                  all_rewards[i*self.batch_size+j] = rewards[j]
   247      3200      24199.0      7.6      0.6                  all_dones[i*self.batch_size+j] = dones[j]
   248      3200      20580.0      6.4      0.5                  all_actions[i*self.batch_size+j] = actions[j]
   249         5         12.0      2.4      0.0          self.image_data = image_data.to(
   250         5      39261.0   7852.2      1.0                  self.device, non_blocking=True).float().split(self.batch_size)
   251         5          7.0      1.4      0.0          self.next_image_data = next_image_data.to(
   252         5       3273.0    654.6      0.1                  self.device, non_blocking=True).float().split(self.batch_size)
   253         5          6.0      1.2      0.0          self.actions = all_actions.to(
   254         5       1191.0    238.2      0.0                  self.device, non_blocking=True).float().split(self.batch_size)
   255         5          5.0      1.0      0.0          self.low_dim_data = low_dim_data.to(
   256         5        220.0     44.0      0.0                  self.device, non_blocking=True).float().split(self.batch_size)
   257         5          5.0      1.0      0.0          self.next_low_dim_data = next_low_dim_data.to(
   258         5        206.0     41.2      0.0                  self.device, non_blocking=True).float().split(self.batch_size)
   259         5          7.0      1.4      0.0          self.rewards = all_rewards.to(
   260         5        212.0     42.4      0.0                  self.device, non_blocking=True).float().split(self.batch_size)
   261         5          6.0      1.2      0.0          self.dones = all_dones.to(
   262         5        203.0     40.6      0.0                  self.device, non_blocking=True).float().split(self.batch_size)

Total time: 6.575 s
File: replay_comparison.py
Function: multi_agent_sampling_test at line 20

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    20                                           @profile
    21                                           def multi_agent_sampling_test(n_agents, tensors):
    22         1         13.0     13.0      0.0      agents = [f"a_{i}" for i in range(n_agents)]
    23         1          1.0      1.0      0.0      replay_buffer = MARLImageReplayBuffer(
    24         1          5.0      5.0      0.0          buffer_size = 500,
    25         1          1.0      1.0      0.0          batch_size = 32,
    26         1          1.0      1.0      0.0          device_name="cuda:2",
    27         1         39.0     39.0      0.0          dimensions=(3, 256, 256)
    28                                               )
    29        21         18.0      0.9      0.0      for agent in agents:
    30        20       1385.0     69.2      0.0          replay_buffer.add_agent(agent)
    31       501        553.0      1.1      0.0      for i in range(500):
    32     10500      13552.0      1.3      0.2          for j, agent in enumerate(agents):
    33     10000       9256.0      0.9      0.1              replay_buffer.add(
    34     10000       7914.0      0.8      0.1                  agent, 
    35                                                           { 
    36     10000      14311.0      1.4      0.2                      "top_down_rgb": tensors[1000*j + i],
    37     10000      39155.0      3.9      0.6                      "low_dim_states": np.random.rand(10)
    38                                                           },
    39     10000      23070.0      2.3      0.4                  np.random.rand(1), 
    40     10000      23474.0      2.3      0.4                  np.random.rand(1),
    41                                                           { 
    42     10000      13768.0      1.4      0.2                      "top_down_rgb": tensors[1000*j + i + 500],
    43     10000      23465.0      2.3      0.4                      "low_dim_states": np.random.rand(10)
    44                                                           },
    45     10000       8483.0      0.8      0.1                  False,
    46     10000    2562120.0    256.2     39.0                  np.random.rand(2)
    47                                                       )
    48         6          9.0      1.5      0.0      for _ in range(N_SAMPLES):
    49       105         88.0      0.8      0.0          for agent in agents:
    50       100        158.0      1.6      0.0              replay_buffer.request_sample(agent)
    51         5    3833542.0 766708.4     58.3          replay_buffer.generate_samples()
    52       105         95.0      0.9      0.0          for agent in agents:
    53       100        504.0      5.0      0.0              replay_buffer.collect_sample(agent)
    54         5         16.0      3.2      0.0          replay_buffer.reset()
    55         1          1.0      1.0      0.0      del(replay_buffer)
    56         1          0.0      0.0      0.0      return

Total time: 0 s
File: replay_comparison.py
Function: single_agent_sampling_test at line 58

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    58                                           @profile
    59                                           def single_agent_sampling_test(n_agents, tensors):
    60                                               agents = {
    61                                                   f"a_{i}": ImageReplayBuffer(
    62                                                       buffer_size = 500,
    63                                                       batch_size = 32,
    64                                                       device_name="cuda:2",
    65                                                       dimensions=(3, 256, 256)
    66                                                   )  
    67                                                   for i in range(n_agents)
    68                                               }
    69                                               for i in range(500):
    70                                                   for j, agent in enumerate(agents):
    71                                                       agents[agent].add(
    72                                                           { 
    73                                                               "top_down_rgb": tensors[1000*j + i],
    74                                                               "low_dim_states": np.random.rand(10)
    75                                                           },
    76                                                           np.random.rand(1), 
    77                                                           np.random.rand(1),
    78                                                           { 
    79                                                               "top_down_rgb": tensors[1000*j + i + 500],
    80                                                               "low_dim_states": np.random.rand(10)
    81                                                           },
    82                                                           False,
    83                                                           np.random.rand(2)
    84                                                       )
    85                                               for _ in range(N_SAMPLES):
    86                                                   for agent in agents:
    87                                                       agents[agent].sample()
    88                                               return 

Total time: 23.3993 s
File: replay_comparison.py
Function: main at line 90

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    90                                           @profile
    91                                           def main():
    92         1    2794590.0 2794590.0     11.9      torch.cuda.empty_cache()
    93         1          4.0      4.0      0.0      times = {}
    94         1        401.0    401.0      0.0      process = psutil.Process(os.getpid())
    95         1          6.0      6.0      0.0      n_agents = list(range(20, 25, 5))
    96         2          5.0      2.5      0.0      for f in ("multi",):
    97         1         22.0     22.0      0.0          pr = Profiler()
    98         1         57.0     57.0      0.0          pr.start()
    99         1          1.0      1.0      0.0          _times = []
   100         2          6.0      3.0      0.0          for n in n_agents:
   101         1         22.0     22.0      0.0              pr.stop()
   102         1         13.0     13.0      0.0              torch.cuda.empty_cache()
   103                                                       tensors = [
   104         1          2.0      2.0      0.0                      np.random.randint(
   105                                                                   0, 255, (3, 256, 256), dtype=np.uint8
   106                                                               )
   107         1   13155008.0 13155008.0     56.2                      for _ in range(1000 * n)
   108                                                       ]
   109         1         53.0     53.0      0.0              pr.start()
   110         1          2.0      2.0      0.0              t = time.time()
   111         1          2.0      2.0      0.0              if f == "multi":
   112         1    6728254.0 6728254.0     28.8                  multi_agent_sampling_test(n, tensors)
   113                                                       else:
   114                                                           single_agent_sampling_test(n, tensors)
   115         1          5.0      5.0      0.0              elapsed = time.time() - t
   116                                                       #t = timeit.timeit(
   117                                                       #    f"{f}_agent_sampling_test({n}, tensors)",
   118                                                       #    setup=f"from __main__ import {f}_agent_sampling_test, tensors",
   119                                                       #    number=1
   120                                                       #)
   121         1          2.0      2.0      0.0              _times.append(elapsed)
   122         1       9918.0   9918.0      0.0              del(tensors)
   123         1          2.0      2.0      0.0          times[f] = _times
   124         1         70.0     70.0      0.0          pr.stop()
   125         1     699383.0 699383.0      3.0          print(pr.output_text(unicode=True, color=True))
   126                                           
   127         1       1310.0   1310.0      0.0      df = pd.DataFrame(times, index=n_agents)
   128         1         52.0     52.0      0.0      test_dir = Path("results", "MARB_testing")
   129         1         68.0     68.0      0.0      test_dir.mkdir(exist_ok=True, parents=True)
   130         1       2946.0   2946.0      0.0      df.to_csv(test_dir / "results.csv", index_label="n_agents")
   131         1       7084.0   7084.0      0.0      print(df)

