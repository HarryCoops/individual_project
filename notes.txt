Notes

10/4/21

For the first profiling we can focus on a single environment and run different models on the environment. We can then see how the complexity of the model impacts the runtime of the training and if its significant compared to the computation involved in simulating the environment.

The first environment can be highway-env0 - the base version.
Iterate through all of the models in configs/HighwayEnv/agents and run each for some number of episodes, say 10 to begin with.
Use the cProfile to get a profile and save it as a CSV, along with the env_config and agent_config.
Structure of the results:
results
   agent_name-env_name
       agent_config.json
       env_config.json
       cProfile.csv

Doing this results in two problems:
 1. Some of the agents are not compatable with all the environment configurations (for example, the attention DQN is not compatible with the default highway env)
 2. Some of the CSVs in the agent configuration folders are not agent_configurations but instead benchmark configurations, which doesn't work with my script in its current state)
Possible fixes:
 1. Edit the script to ignore agent exceptions (currently doing that) and also benchmark configurations (not currently doing)
 2. Use the benchmark and evaluate scripts provided with rl-agents and manually specify all of the agent / environment configurations
I think (2) might be better.
In order to do (2) I need to get a better idea of which agents work with which envs, I might need to read some papers to get this knowledge.
